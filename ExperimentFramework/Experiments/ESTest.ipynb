{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/edwardgunn/Documents/4YP/DistributedReinforcementLearningOnTheEdge\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running algorithm: EvolutionStrategies\n",
      "Running 20 agents\n",
      "Running environment: LunarLander-v2\n",
      "Starting training\n",
      "\n",
      "Episode 0/10\n",
      "Episode 1/10\n",
      "Episode 2/10\n",
      "Episode 3/10\n",
      "Episode 4/10\n",
      "Episode 5/10\n",
      "Episode 6/10\n",
      "Episode 7/10\n",
      "Episode 8/10\n",
      "Episode 9/10\n",
      "\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m parameters \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mnumbersOfAgents\u001b[39m\u001b[39m\"\u001b[39m:[\u001b[39m20\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mnumEpisodes\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m10\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmaxSteps\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m100000\u001b[39m}\n\u001b[1;32m     29\u001b[0m universe \u001b[39m=\u001b[39m Universe(typesOfEnvironment, environmentParameters, algorithms, algorithmParameters, parameters)\n\u001b[0;32m---> 30\u001b[0m universe\u001b[39m.\u001b[39;49mrunTrial()\n\u001b[1;32m     31\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mExperimentFramework/Experiments/Results/11.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m outp:\n\u001b[1;32m     32\u001b[0m     pickle\u001b[39m.\u001b[39mdump(universe\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdata, outp, pickle\u001b[39m.\u001b[39mHIGHEST_PROTOCOL)\n",
      "File \u001b[0;32m~/Documents/4YP/DistributedReinforcementLearningOnTheEdge/ExperimentFramework/Universe.py:43\u001b[0m, in \u001b[0;36mUniverse.runTrial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironments \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironmentFactory\u001b[39m.\u001b[39mmakeEnvironments(numAgents)\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 43\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunTraining(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparameters[\u001b[39m\"\u001b[39;49m\u001b[39mnumEpisodes\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/Documents/4YP/DistributedReinforcementLearningOnTheEdge/ExperimentFramework/Universe.py:54\u001b[0m, in \u001b[0;36mUniverse.runTraining\u001b[0;34m(self, iterations)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpisode \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters[\u001b[39m'\u001b[39m\u001b[39mnumEpisodes\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcentralLearner\u001b[39m.\u001b[39mnextEpisode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironments[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mgetEnvironmentInfo())\n\u001b[0;32m---> 54\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstart()\n",
      "File \u001b[0;32m~/Documents/4YP/DistributedReinforcementLearningOnTheEdge/ExperimentFramework/Universe.py:69\u001b[0m, in \u001b[0;36mUniverse.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstepNumber: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning:\n\u001b[0;32m---> 69\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     70\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mlogStep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstepNumber)\n\u001b[1;32m     71\u001b[0m     \u001b[39mif\u001b[39;00m maxSteps \u001b[39m:=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmaxSteps\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/4YP/DistributedReinforcementLearningOnTheEdge/ExperimentFramework/Universe.py:59\u001b[0m, in \u001b[0;36mUniverse.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mfor\u001b[39;00m environment \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironments:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m environment\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     60\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcentralLearner\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstepNumber \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/4YP/DistributedReinforcementLearningOnTheEdge/ExperimentFramework/Environments/GymEnvWrapper.py:27\u001b[0m, in \u001b[0;36mGymEnv.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning:\n\u001b[1;32m     26\u001b[0m     \u001b[39mfor\u001b[39;00m agent \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents:\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrentObservation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrentReward, terminated, truncated, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvironment\u001b[39m.\u001b[39;49mstep(agent\u001b[39m.\u001b[39;49mgetAction())\n\u001b[1;32m     28\u001b[0m         agent\u001b[39m.\u001b[39mstep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrentObservation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrentReward)\n\u001b[1;32m     29\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m (terminated \u001b[39mor\u001b[39;00m truncated)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:51\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     41\u001b[0m     \u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     52\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gymnasium/envs/box2d/lunar_lander.py:605\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    602\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m+\u001b[39m\u001b[39m100\u001b[39m\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 605\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    606\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(state, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32), reward, terminated, \u001b[39mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gymnasium/envs/box2d/lunar_lander.py:657\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[39mfor\u001b[39;00m coord \u001b[39min\u001b[39;00m p:\n\u001b[1;32m    656\u001b[0m         scaled_poly\u001b[39m.\u001b[39mappend((coord[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m SCALE, coord[\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m SCALE))\n\u001b[0;32m--> 657\u001b[0m     pygame\u001b[39m.\u001b[39;49mdraw\u001b[39m.\u001b[39;49mpolygon(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msurf, (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m), scaled_poly)\n\u001b[1;32m    658\u001b[0m     gfxdraw\u001b[39m.\u001b[39maapolygon(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf, scaled_poly, (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m))\n\u001b[1;32m    660\u001b[0m \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparticles \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrawlist:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from ExperimentFramework.Algorithms.EvolutionStrategies import EvolutionStrategies\n",
    "from ExperimentFramework.Universe import Universe\n",
    "from ExperimentFramework.Environments.BlackJack import BlackJack\n",
    "from ExperimentFramework.Environments.CliffWalking import CliffWalking\n",
    "from ExperimentFramework.Environments.FrozenLake import FrozenLake\n",
    "from ExperimentFramework.Environments.Taxi import Taxi\n",
    "from ExperimentFramework.Environments.LunarLander import LunarLander\n",
    "from Common.JsonEncoder import NpEncoder\n",
    "\n",
    "from ExperimentFramework.Environments.SimpleGrid import SimpleGrid\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "typesOfEnvironment = [LunarLander]\n",
    "environmentParameters = [{}]#[{\"render_mode\":\"human\"}]\n",
    "algorithms = [EvolutionStrategies]\n",
    "algorithmParameters = [{\n",
    "    \"centralLearnerParameters\":{\n",
    "        \"alpha\":0.01\n",
    "        }, \n",
    "    \"agentParameters\":[{}],\n",
    "    \"maxSeedInt\": 1000000,\n",
    "    \"seed\": 5,\n",
    "    \"sigma\": 1,\n",
    "    \"hiddenSize\": 8\n",
    "}]\n",
    "parameters = {\"numbersOfAgents\":[20], \"numEpisodes\":10, \"maxSteps\": 100000}\n",
    "universe = Universe(typesOfEnvironment, environmentParameters, algorithms, algorithmParameters, parameters)\n",
    "universe.runTrial()\n",
    "with open('ExperimentFramework/Experiments/Results/11.pkl', 'wb') as outp:\n",
    "    pickle.dump(universe.logger.data, outp, pickle.HIGHEST_PROTOCOL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
